\documentclass[letterpaper, 12pt]{article}
\usepackage[letterpaper, portrait, margin=1in]{geometry}
\usepackage{multicol}
\usepackage[english]{babel}
\usepackage{graphicx}

\usepackage{xcolor}
\usepackage{blindtext}

\usepackage[
backend=biber,
natbib=true,
% style=apa,
style=ieee
]{biblatex}
\addbibresource{references.bib}

% Paper requirement 1.5 line spacing. Remove for github publishing
\usepackage{setspace}
\onehalfspacing

\title{Automatic Singing Transcription: Phonemes and Alignment}
\author{Victor Shan}


\begin{document}
\maketitle
\begin{multicols*}{2}
\begin{abstract}
\end{abstract}

\section{Introduction}

Songs play an important part of society because they are conduits of cultural expression,
foster emotional connection and is a great source of entertainment. Automatic Singing Transcription
(AST) is a branch of Automatic Speech Recognition (ASR) can facilitate a deeper way to interact with
it. In ASR, the goal is to transcribe normal speech to text,
whether that's a conversation between two people, a lecture, or a podcast. Examples of ASR include
Youtube's auto-generated captions, assistants like Siri and Alexa, and even live captioning of
meetings through applications like Zoom. AST is a similar task, but instead of transcribing normal
speech, AST transcribes singing. Applications of AST range from karaoke applications to Music
Information Retrieval (MIR) systems. These MIR systems can be used to search for songs based on
lyrics, categorize songs based on their lyrics, or even aid in the generation of new songs.

While AST is similar to ASR, there are a few key differences that make it a more challenging task.
First, singing is a more complex signal than normal speech. Singing has a more dynamic signal, with
more variation in pitch, duration and vibrato \citep{SemiSupervisedSolo}.
For example, vowels sounds are often held for longer in singing than in normal speech like dragging
the last part of ``bye" in word ``goodbye". Songs also often include music in the background, which can
make it more difficult to distinguish between the singer's voice and the background music. Finally,
there is a lack of large, publicly available datasets for AST \citep{DALI}. This is in contrast to ASR, where
there are many large datasets available, such as LibriSpeech \citep{Librispeech}. This lack of data
makes it especially difficult to train AST models.

There are many possible outputs for AST but one of the most detailed and useful outputs is the
time-aligned phoneme sequence. This is a sequence of phonemes, or the smallest unit of sound in a
language, that is aligned with the audio. Time-aligned phoneme sequences are useful because it can contains enough
information to reconstruct the not only the lyrics, but also how they match with the music. This
output can be used to generate a karaoke application that highlights the lyrics as they are sung,
or even train a model to sing covers of songs. Word-level alignement is an easier alternative but it
misses the crucial information of the duration of phonemes within words. The ideal output is one that includes even more information
such as the notes to sing at so that singers would be able to read and sing the song without any
additional information. In this paper, we will mainly focus on the phoneme-level alignment but use
word-level alignment to compare the performance of our model to other models.


\section{Datasets}

\subsection{Requirements}
The requirements of a good dataset are audio files, lyrics and timestamps. The audio files can be
in any format such as mp3 or wav. The lyrics can be in any format as well, but the timestamps must
be in a format that can be used to align the lyrics with the audio. The timestamps can be in the
form of a phoneme sequence or word sequence but should also include the onset and offeset times
(beginning and end). Sentence level timestamps are too coarse and would need
to be broken down into word or phoneme level timestamps. Most lyrics used by popular music services
such as Spotify only use sentence/line level timestamps. Ideally, the dataset would also be large
enough to train deep neural networks and publicly available so that other researchers can use it and
compare their models to existing models.

\subsection{Challenges}
With these requirements in mind, it is easy to see why there are so few datasets available. First,
it is difficult to create a dataset with lyrics and timestamps. The lyrics must be manually
transcribed and the timestamps must be manually aligned with the audio. This is a time consuming
process that requires a lot of effort. Second, it is difficult to obtain the rights to use the
audio files. Most songs are owned by record labels and it is difficult to obtain the rights to use
these files. These two challenges make it difficult to create new datasets and is the reason why
there are so few datasets available and why most of the existing datasets are small.

\subsection{Dataset Augmentation}
\subsubsection{SpecAugment}
Due to the lack of datasets, it is more important to make the most of the existing datasets. One
way to do this is to augment the existing datasets. SpecAugment is a series of techniques that
augment the audio spectrogram to improve the performance of ASR models. A common variation of this
representation is the Mel-Frequency Cepstral Coefficient (MFCC) spectrogram.

\begin{figure*}
    \centering
    \includegraphics[width=0.4\textwidth]{assets/waveform-to-spectrogram-sing.png}
    \includegraphics[width=0.4\textwidth]{assets/melspectrogram-mfcc.png}
    \caption{Waveform (top left) can be transformed into a spectrogram (bottom left).
    The spectrogram can be transformed to use the Mel scale in Mel Spectrogram (top right,
    only first section shows) which can futher be transformed into a more discrete MFCC.
    Figures inspired by torchaudio \citep{torchAudioFigures}.}
    \label{fig:MFCC}
\end{figure*}

These are image representations of
energy at certain frequencies on a scale that more closely matches human hearing \citep{MFCC}.
These techniques include time warping, frequency masking and time masking \citep{SpecAugment}.
Time warping is a technique that stretches or compresses the audio spectrogram in the time
dimension. Frequency masking is a technique that masks a random number of frequency channels in
the audio spectrogram. Time masking is a technique that masks a random number of time steps
in the audio spectrogram. These techniques are used to augment the audio spectrogram before it
is fed into the ASR model. \citep{SpecAugment}

{
    \centering
    \includegraphics[width=0.45\textwidth]{assets/specaugment.png}
}

\subsubsection{Transforming Existing Datasets Into Pseudo-Singing Datasets} \label{sec:pseudoSinging}
Another way to augment the existing datasets is to transform the existing datasets. This can be
done by shifting the pitch, duration or vibrato \citep{SongifiedSpeech}. The advantage of this
technique is that is can also be applied to speech datasets and transform them into pseudo-singing
datasets. The disadvantage is that the results will contain artifacts from the transformation based
on the techiniques applied. Neural network models showed an almost 15\% improvement on the
transformed TIMIT dataset than ones trained on the original TIMIT dataset \citep{SongifiedSpeech}.

\subsubsection{Transforming Utterance level datasets into Phoneme level datasets} \label{sec:utteranceToPhoneme}
A technique from ASR that can also be applied to AST is transforming utterance/sentence
level datasets into phoneme level datasets. There are many ASR datasets that contain single utterances
such as LibriSpeech and JamendoLyrics. These datasets can be effectively
transformed into phoneme level datasets by using a phoneme dictionary such as the CMU Pronouncing
Dictionary \citep{CMUDict}. This dictionary contains a mapping from words to
ARPAbet style phonemes \citep{CMUDict}. Using this
dictionary, the utterances can be transformed into phoneme sequences. These sequences can then be
used with a Connectionist Temporal Classification (CTC) loss function to train AST models \citep{CTC}.
\begin{figure*}
    \centering
    \includegraphics[width=0.5\textwidth]{assets/CTC.png}
    \caption{Example of CTC Function collapsing repetitions. Image from \citep{CTC}}
    \label{fig:CTC}
\end{figure*}
CTC allows for the model to output a sequence of phonemes per time step and the
repetitions are collapsed into a single phoneme as shown in Figure \ref{fig:CTC}.
This technique was used to allow a model to incorperate the utterance level LibriSpeech
dataset into the training for a phoneme ASR model \citep{wav2vec}. Timestamps can be
retrieved from the pre-CTC output that had time-aligned phoneme classifications.
The same process can be applied to AST models to allow them to use the utterance level song datasets.

\subsubsection{Teacher-Student Approach} \label{sec:TeacherStudent}
The teacher-student approach is inspired by the technique of the same name that was intended
to reduce the size of large deep neural networks. The idea was first train a large deep neural
network, the teacher, and then train a smaller deep neural network, the student, to mimic the
teacher \citep{TeacherStudent}. However, this technique can also be applied in cases of low labeled
data availablility but high unlabeled data availablility. To start off, a model would be
trained on a small dataset of labeled data. Then, the model would be used to label a large dataset
of unlabeled data. Finally, a new model would be trained on the newly labeled dataset
\citep{DALI}. This has proven to be effective for transcribing drums in music with the student
model outperforming the teacher model \citep{DrumsStudentTeacher}.

\subsection{Existing Datasets} \label{sec:datasets}

\subsubsection{TIMIT (ASR)}
TIMIT is a dataset of speech recordings of 630 speakers of eight major dialects of American English
with time-aligned phoneme sequences \citep{TIMIT}. It is a popular dataset for ASR and has been used
to train many ASR models. However, since this is not a singing dataset, it does not contain any of
the characteristics of singing such as pitch, duration or vibrato. This is an excellent dataset to
apply transformation into a pseudo-singing dataset metioned in \ref{sec:pseudoSinging}. The popularity of this dataset
makes it a good candidate for applying transformations to create a pseudo-singing dataset and also
for a general benchmark to compare against other models.

\subsubsection{LibriSpeech (ASR)}
LibriSpeech is a 1000 hour dataset of audiobook recordings where each recording has a matching sentence \citep{Librispeech}. This
dataset is also a popular dataset for ASR and has been used to train many ASR models including
recent breakthroughs like wav2vec 2.0 \citep{wav2vec}. This dataset is also a good candidate for
training AST models because it is large and publicly available. This dataset is special because of
the clarity of the audio and previous success by other models such as the wav2vec 2.0 model in
detecting phonemes in this dataset when fine tuned with the TIMIT dataset \citep{wav2vec}. This is
done through the CTC technique mentioned in \ref{sec:utteranceToPhoneme}. Since
speech and singing both use the same phonemes, because they are in the same language, this large
dataset can be used to train base models before fine-tuning them AST models.

\subsubsection{Jamendo Dataset}
This dataset one of the most popular datasets for AST and has been used by many state-of-the-art
AST models. It contains 20 English songs and 60 songs in other languages with word-aligned timestamped sequences \citep{JamendoLyrics}. This dataset
is a good candidate for training AST models because it is publicly available and it's popularity
makes it an excellent benchmark to compare against other models. However, it is still a relatively
small dataset and does not contain any phoneme sequences. The authors of this dataset were able to
achieve a 77.8\% Word Error Rate (WER) which still leaves a lot of room for improvement.

\subsubsection{Children's Songs Dataset}
This is a dataset of 50 English and 50 Korean children's songs with grapheme level timestamps and
pitches. This dataset was sung by a single professional Korean singer in two separate keys for each
song \citep{CSD}. This dataset is a good candidate for training AST models because it
includes the onset and offset information, pitch, and grapheme level timestamps that could be
preprocessed to phoneme or word level timestamps. It is also a very clean dataset because of the
professional performance and setting. The downside of this dataset is that graphemes are
not as popular as phonemes, word or utterance labels in either ASR or AST.

\subsubsection{MUSDB18}
MUSEDB18 is a dataset of 150 songs with isolated vocals and accompaniment tracks \citep{musdb18}. This
dataset is a good candidate for training AST models because it is publicly available and it has
clean isolated vocals. The downside of this dataset is that it doesn't contain any word level
timestamps. However, with the CTC technique mentioned in \ref{sec:utteranceToPhoneme}, this dataset
can still be used to train AST models.

\subsubsection{NUS Dataset}
This dataset is one of the few datasets that contains phoneme level timestamps \citep{NUSDataset}.
There are 169 minutes of 20 unique English songs by 12 different people. The CMUDict was used
for the phoneme vocabulary and timestamps were manually annotated. This dataset is the ideal dataset
type for training AST models due to this level of detail. It also includes a mix of slow to fast
melodies annd a balanced gender distribution \citep{NUSDataset}.


\subsubsection{Free Music Archive}
The Free Music Archive (FMA) dataset is a dataset of 106,574 tracks with 161 genres \citep{FMA}. This
dataset is not a good candidate for direclty training AST models because it does not contain any lyrics
and some music may not even contain any singing at all. However, it is a good source of unlabeled
songs that could be labeled through the teacher-student technique in \ref{sec:TeacherStudent}. It
can also provide a good source of general singing audio that can be used in the
training of wav2vec2.0 models that will be described in section \ref{sec:wav2vec}.

\subsubsection{VocalSet}
VocalSet is a 10 hour dataset of a capella singing from 20 professional singers
demonstrating a variety of singing techniques \citep{VocalSet}. This dataset is a good candidate for
training Voice Activity Detection (VAD) models because it contains onset and offset timestamps for
each vocal segment. This is also a good a good dataset to help train AST models to know what singing
sounds like.

\subsubsection{Other Datasets}
Many datasets were considered but left out due to the lack of availablility. Some of the most
popular datasets such as Mauch's Dataset \citep{mirex2021} and Hansen's Dataset \citep{Hansen} are
not publicly available anymore. Newer datasets such as DALI \citep{DALI} and DAMP! \citep{DAMP} are
are hidden behind institutional logins and require manually requesting access.

While both the Mauch's Dataset and Hansen's Datasets are quite small (Mauch has 20 songs, Hansen has
9 \citep{mirex2021}), the DALI and DAMP! datasets are much larger. The DALI dataset in particular
used a version of the Teacher Student technique mentioned in \ref{sec:TeacherStudent} to label 105
songs with timestamps for the word and phoneme level \citep{DALI}. The DAMP! dataset is even larger
with 300x30x2 song dataset. Both of these datasets would be excellent candidates for training AST
models from their size alone.

\section{Related Works}
\subsection{HMM Based Acoustic Models}
The traditional approach to AST is can be separated into a pipeline of distinct steps. The first
step is to extract the features from the audio. This usually includes some form of spectral analysis
such as MFCCs. The second step is to use an acoustic model to classify the features into phonemes
and generate a sequence of phonemes using a Hidden Markov Model (HMM). This is the traditional
approach to ASR and is also used in AST \citep{Hansen}.

{
\color{red}
Other models focus on the transcription of notes instead of lyrics. These models use a similar
to the ones that are transcribing lyrics
TODO: Add more detail about HMMs
TODO: Write about Dynamic Time Warping
TODO: Write about some results of HMMs
}

% \subsection{Music Informed Models}

\subsection{wav2vec 2.0 and Transfer Learning} \label{sec:wav2vec}

\subsubsection{wav2vec 2.0}
One of the most recent breakthroughs in ASR is wav2vec 2.0 \citep{wav2vec}. This model uses a
self-supervised learning approach was used during the training of the model. Unlabeled audio was
fed into the model to learn discrete speech units. These discrete speech units required the
Gumbel-Softmax \citep{gumbelSoftmax} to allow for backpropagation. The model was then
fine-tuned with a linear layer and CTC loss on labeled data to perform ASR \citep{wav2vec}.

This approach was able to achieve state-of-the-art results on the LibriSpeech dataset \citep{Librispeech}
and the TIMIT dataset \citep{TIMIT}. However, the thing that makes this approach the most promising
is the fact that after pre-training on a large amount of unlabeled data, the model can be fine-tuned
on a 10 minute subset of labeled data to achieve 5.2 WER on the LibriSpeech clean dataset \citep{wav2vec}.
This is very important for AST because there are so few labeled datasets available.

\subsubsection{wav2vec 2.0 Transfer Learning}
Using transfer learning for AST using wav2vec 2.0 was attempted in 2022 and was used to achieve
state-of-the-art results on the Jamendo dataset as well as on the DALI, Hansen, Mauch,
and DAMP! datasets \citep{wav2vecTransfer}. The approach they performed did not change the first part
of the model that generated the discrete speech units. Instead, they changed the last part of the
model to have another branch that outputs the probability of the current word given previous words
and context. This approach achieved a 33.13 WER on the Jamendo dataset \citep{wav2vecTransfer}.

Apart from their results, there were also a few other interesting things about their approach. Since
the initial wav2vec 2.0 model was trained on the LibriSpeech dataset, they wanted to make the input
audio of their model as similar to the LibriSpeech dataset as possible. To do this, they used
Demucs v3 \citep{Demucs} to separate the vocals from the accompaniment. They did not futher remove any
noise or singing specific features from the audio. For their labeled datasets, they used
utterance level labels excluding the instrumental parts. For the output, they used character level
labels.

\subsection{Whisper and WhisperX Word-Level Alignment}

\subsubsection{Whisper}
Whisper a ASR model trained on a extremely large dataset of 680,000 hours of audio \citep{whisper}.
Instead of using self-supervised learning like in wav2vec 2.0, Whisper uses a weakly supervised
approach. This approach uses a large dataset of audio and text that is not high quality as many of
the smaller datasets. Instead the dataset was generated by scraping the internet for audio and
their corresponding transcriptions. Although Whisper did not improve the state-of-the-art on the
LibriSpeech dataset when compared against wav2vec 2.0, did achieve improvements and therefore
state-of-the-art results on the a variety of other speech datasets \citep{whisper}. The model
performs well in zero-shot scenarios so it can achieve good results without fine-tuning on the
target dataset \citep{whisper}. This is important for AST because there are so many genres and
types of music and it would be difficult to fine-tune on all of them.

\subsubsection{WhisperX}
The original Whisper model was also able to provide some timestamps but they were not the focus of
the paper. It was more of a proof of concept that the same model can be used for multiple tasks
\citep{whisper}. WhisperX is a model that was trained specifically for word-level alignment and
the speedup of the Whisper model \citep{whisperX}.

On the TED-LIUM dataset, a dataset of TED talks \citep{tedlium3}, WhisperX was able to acheive a
11.8x speedup over Whisper. This is comparable to wav2vec 2.0 which was a 10.3x speedup over
Whisper. However, WhisperX was also able to achieve a WER of 9.7 compared to the 10.5 WER from
Whisper and 19.8 WER from wav2vec 2.0 \citep{whisperX}. This is a significant improvement over
of speed with a slight increase in performance. This is important for AST because it would allow
for faster training and inference times.

For the word alignment task, WhisperX uses an external phoneme alignment approach to generate
word level timestamps using phoneme level timestamps \citep{whisperX}. The external phoneme
alignment model was actually a wav2vec 2.0 model that used Dynamic Time Warping (DTW) \citep{DTW}
to align the phonemes \citep{whisperX}. The first and last phonemes of each word were used to
generate the word level timestamps \citep{whisperX}.

The most important improvement of WhisperX over Whisper is VAD Cut \& Merge technique. This
technique uses a Voice Activity Detection (VAD) model to cut the audio into segments at low voice
activity sections. This decreases the hallucination of Whisper if a word was partly spoken in a
audio segment which lead to the slight increase in WER \citep{whisperX}. The speedup of WhisperX
was also due to this technique it allowed parallel processing of the audio segments where
Whisper had to process sequenctially \citep{whisperX}.


\section{Method}
In order to produce a model that can perform AST, we train a model to first take the audio and align
it with the lyrics like in the last step of the WhisperX pipeline. This helps us create a model that
can poperly annotate phonemes from the audio. Then, a secondary model can be trained to take the
audio and the phoneme annotations to generate the lyrics. With both a lyric and phoneme generator,
any song will be able to be transcribed into lyrics and time-aligned phonemes for an end-to-end AST.

Each step in this process allows for the use of teacher-student techniques mentioned in section
\ref{sec:TeacherStudent} to generate more labeled data from unlabeled data. This is important
because there are so few labeled datasets available for AST.

\subsection{Preprocessing Datasets}
To preprocess datasets, the audio files were preprocessed in the same way WhisperX preprocessed
data with the VAD Cut \& Merge technique \citep{whisperX} except with a slight modification before
trying to detect voices. Since the audio files were songs, most of them will have both singing and
background music. This means that the VAD model will have a harder time detecting when the singing
stops. To help the VAD model, the audio files were first passed through Demucs v4 \citep{Demucs}
to separate the vocals from the background music. Then, the VAD model was used to detect the
voices. Finally, the audio files were cut into segments at the detected voice sections. To reduce
memory requirements and because lyrics utterances are often shorter than 30 seconds, the segments
will be cut into 5-15 second segments. Smaller segments can either be padded
or merged with other segments into larger ones.

The first subsystem of our AST model will be to take the audio and generate the phoneme annotations.
This subsystem will be trained and tested on the TIMIT, Children's Song and NUS datasets. These are
all datasets that contain phoneme level annotations. TIMIT, although it is a speech dataset, can be
transformed into a pseudo-singing dataset as mentioned in \ref{sec:pseudoSinging} or used directly
as a different variety of vocalizing phonemes.

In order to transform these datsets into a valid input for the model, datasets that only contain
utterances will be transformed into phoneme level datasets using the CMU Pronouncing Dictionary
(CMUDict) and then tokenized. Luckily, the TIMIT dataset already uses an extended version of the
ARPAbet phonemes so they can be remapped into a smaller subset of phonemes. The CMUDict also
includes stresses in the form of numbers at the end of some phonemes that were removed. Finally,
the unknown and padding tokens were added to the phoneme vocabulary. Any words that appeared in
the datasets but were not in CMUDict were manually added to the dictionary. HuggingFace and PyTorch
constructs to help with this process and allow for batching.

\subsection{Augmenting Datasets}
There are many ways to augment datasets. The three ways mentioned in SpecAugment includes
time warping, frequency masking and time masking \citep{SpecAugment}. These techniques can be
applied to the audio spectrogram before it is fed into the model. Time warping is a technique that
stretches or compresses the audio spectrogram in the time dimension due to the complexity of
realigning the audio. We use time and frequency maksing for training our models.

% \subsection{Combining Datasets}

\section{Training}

\subsection{Base Model}
In order to get a baseline for our improvements, we train a base model on the TIMIT dataset. This
This dataset is a one of the most popular and defacto standard for ASR models that do phoneme
recognition. The starting model that will be finetuned on TIMIT will be the `facebook/wav2vec2-base-960h'
model from the HuggingFace \citep{huggingFaceWav2vec2}.

\subsection{Singer Model}

% \begin{enumerate}
%     \item Preprocess datasets
%     \item Augment datasets
%     \item Create frankenstein dataset
%     \item Train model
%     \item Evaluate model
%     \item Label unlabeled datasets
%     \item train student model on newly labeled datasets
%     \item evaluate student model on original, manually labeled datasets
%     \item repeat
% \end{enumerate}

\section{Results}

% \subsection{VAD Performance}
\subsection{Phoneme Recognition Performance}

\subsection{Phoneme Alignment Performance}
% PER

% \subsection{Lyric Generation Performance}
% % WER

\section{Discussion}

\section{Future Work}
\subsection{Train on More Datasets}
Dispite the lack of datasets, there are still many datasets listed in section \ref{sec:datasets}
that can be used to train AST models. Deep learning networks are known to perform better with more
training data and the greater variety of data will allow the model to generalize better.
datasets used in the training of our models were TIMIT and CSD. However, there are many genres of
music that are not represented in children's songs such as rap.

\subsection{Finetuning Wav2Vec 2.0 Feature Extractor}
The FMA dataset described in section \ref{sec:datasets} can be used to finetune the wav2vec 2.0
feature extractor. This will allow the model to learn features that are more specific to singing
and music. This dataset will need some extra preprocessing to detect singing because the songs
include music with no vocals. This can be done by using a VAD model to detect the singing and
removing the parts of the audio that do not contain singing. The wav2vec 2.0 paper mentions how
the model goes through self-supervised learning to learn discrete speech units \citep{wav2vec} and
a large dataset like FMA will allow the model to learn discrete singing units.

\subsection{Generative Adversarial Networks}
Generative Adversarial Networks (GANs) are a type of neural network that can be used to generate
new data and can have recently become popular with the success of Midjourney and
Stable Diffusion. Voice synthesis and conversion have also improved with services that
clone voices or text-to-speech services. The intersection of these technologies can
be used to generate new songs. The songs that are generated do not need to be top highlights
but plausible sounding songs that can be used to train AST models. Using a GAN can generate
songs with more variety than the pitch shifted songs mentioned in \ref{sec:pseudoSinging} and
can lead to better generalization.

\subsection{Phoneme to Grapheme}
One of the main applications of AST is to generate and sync lyrics to songs for karaoke
applications. However, most people cannot read ARPAbet phonemes or IPA phonemes. Instead,
they can read words and graphemes that form words. Phonemes and graphemes represent the same
unit of sound but the graphemes are more readable because they are just chunks of the original word.
This means that as the words are sung, each grapheme of the word can be highlighted individually.
To get a grapheme output, the phoneme output can be converted to a grapheme output using a
separate phoneme to grapheme model or generate character level output and align the characters and
phonemes using the timestamps.

\section{Conclusion}
\citep{pyannote}
\printbibliography

\end{multicols*}

\end{document}