{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/miniconda3/envs/song-transcription/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR already in sys.path ['/mnt/storage/projects/programming/song-transcription/code/alignment', '/home/victor/miniconda3/envs/song-transcription/lib/python311.zip', '/home/victor/miniconda3/envs/song-transcription/lib/python3.11', '/home/victor/miniconda3/envs/song-transcription/lib/python3.11/lib-dynload', '', '/home/victor/miniconda3/envs/song-transcription/lib/python3.11/site-packages', '/home/victor/miniconda3/envs/song-transcription/lib/python3.11/site-packages/huggingface_hub-0.19.4-py3.8.egg', '/tmp/tmps0c7juft', '/mnt/storage/projects/programming/song-transcription/data']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/miniconda3/envs/song-transcription/lib/python3.11/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "/home/victor/miniconda3/envs/song-transcription/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../../../home/victor/.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.1.1. Bad things might happen unless you revert torch to 1.x.\n",
      "DATA_DIR already in sys.path ['/mnt/storage/projects/programming/song-transcription/code/alignment', '/home/victor/miniconda3/envs/song-transcription/lib/python311.zip', '/home/victor/miniconda3/envs/song-transcription/lib/python3.11', '/home/victor/miniconda3/envs/song-transcription/lib/python3.11/lib-dynload', '', '/home/victor/miniconda3/envs/song-transcription/lib/python3.11/site-packages', '/home/victor/miniconda3/envs/song-transcription/lib/python3.11/site-packages/huggingface_hub-0.19.4-py3.8.egg', '/tmp/tmps0c7juft', '/mnt/storage/projects/programming/song-transcription/data']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "parent_dir = Path(sys.path[0])\n",
    "data_dir = parent_dir.parent.parent / 'data'\n",
    "if str(data_dir) not in sys.path:\n",
    "    sys.path.append(str(data_dir))\n",
    "from timit.timit import get_timit\n",
    "from nus.nus import get_nus\n",
    "from jamendolyrics.jamendolyrics import get_jamendo\n",
    "from CMUdict.utils import CMUDict, VOCAB\n",
    "# Inspired by https://www.kaggle.com/code/vitouphy/phoneme-recognition-with-wav2vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'text', 'phonemes'],\n",
       "        num_rows: 3696\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'text', 'phonemes'],\n",
       "        num_rows: 1680\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'text', 'phonemes'],\n",
       "        num_rows: 924\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timit_dataset = get_timit()\n",
    "timit_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They assume no burglar will ever enter here.\n",
      "SIL DH EY IH S UW M N OW SIL B ER SIL G L ER W L SIL EH V ER SIL EH N SIL T ER HH IH ER SIL\n",
      "{'UH': 0, 'AW': 1, 'R': 2, 'SH': 3, 'OY': 4, 'AY': 5, 'UW': 6, 'P': 7, 'T': 8, 'B': 9, 'L': 10, 'S': 11, 'AH': 12, 'M': 13, 'D': 14, 'IH': 15, 'OW': 16, 'G': 17, 'IY': 18, 'DX': 19, 'K': 20, 'W': 21, 'SIL': 22, 'AE': 23, 'EH': 24, 'V': 25, 'EY': 26, 'NG': 27, 'CH': 28, 'ER': 29, 'N': 30, 'F': 31, '[UNK]': 32, 'Z': 33, 'HH': 34, 'JH': 35, 'DH': 36, 'Y': 37, '[PAD]': 38, 'AA': 39, 'TH': 40, '|': 41}\n",
      "42 {'SH', 'UH', 'AW', 'R', 'OY', 'AY', 'UW', 'P', 'T', 'B', 'L', 'S', 'AH', 'M', 'D', '|', 'IH', 'OW', 'G', 'IY', 'DX', 'K', 'W', 'SIL', 'AE', 'EH', 'V', 'EY', 'NG', 'CH', 'ER', 'N', 'F', '[UNK]', 'Z', 'HH', 'JH', 'DH', 'Y', '[PAD]', 'AA', 'TH'}\n"
     ]
    }
   ],
   "source": [
    "def change_item(item):\n",
    "    item['phonetic'] = \" \".join(item['phonemes'])\n",
    "    return item\n",
    "timit_dataset = timit_dataset.map(change_item)\n",
    "# timit_dataset = timit_dataset.remove_columns(['phonemes'])\n",
    "timit_row = timit_dataset['train'][0]\n",
    "print(timit_row['text'])\n",
    "# print(timit_row['phonemes'])\n",
    "print(timit_row['phonetic'])\n",
    "vocab = set(VOCAB.keys())\n",
    "print(VOCAB)\n",
    "print(len(vocab), vocab)\n",
    "with open('working/vocab.json', 'w') as f:\n",
    "    json.dump(VOCAB, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained('working/', unk_token='[UNK]', pad_token='[PAD]', word_delimiter_token='|')\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.tokenize(timit_row['phonetic'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"return_attention_mask\": true,\n",
       "  \"sampling_rate\": 16000\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16_000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Processor:\n",
       "- feature_extractor: Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"return_attention_mask\": true,\n",
       "  \"sampling_rate\": 16000\n",
       "}\n",
       "\n",
       "- tokenizer: Wav2Vec2CTCTokenizer(name_or_path='working/', vocab_size=42, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'pad_token': '[PAD]', 'additional_special_tokens': [AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True)]}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3696/3696 [02:30<00:00, 24.49 examples/s]\n",
      "Map: 100%|██████████| 1680/1680 [01:08<00:00, 24.68 examples/s]\n",
      "Map: 100%|██████████| 924/924 [00:37<00:00, 24.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\"\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"phonetic\"]).input_ids\n",
    "    return batch\n",
    "timit_dataset = timit_dataset.map(prepare_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Copied from: https://www.kaggle.com/code/vitouphy/phoneme-recognition-with-wav2vec2s\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "wer_metric = load_metric(\"wer\")\n",
    "cer_metric = load_metric(\"cer\")\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids)\n",
    "    label_str = tokenizer.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\n",
    "        \"wer\": wer,\n",
    "        \"cer\": cer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized because the shapes did not match:\n",
      "- lm_head.bias: found shape torch.Size([32]) in the checkpoint and torch.Size([44]) in the model instantiated\n",
      "- lm_head.weight: found shape torch.Size([32, 768]) in the checkpoint and torch.Size([44, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=768, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-base-960h\",\n",
    "    attention_dropout=0.1,\n",
    "    layerdrop=0.0,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.75,\n",
    "    mask_time_length=10,\n",
    "    mask_feature_prob=0.25,\n",
    "    mask_feature_length=64,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "model.freeze_feature_extractor()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 755574 KiB |   1487 MiB |   2033 GiB |   2032 GiB |\n",
      "|       from large pool | 753920 KiB |   1486 MiB |   1981 GiB |   1980 GiB |\n",
      "|       from small pool |   1654 KiB |      5 MiB |     52 GiB |     52 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 755574 KiB |   1487 MiB |   2033 GiB |   2032 GiB |\n",
      "|       from large pool | 753920 KiB |   1486 MiB |   1981 GiB |   1980 GiB |\n",
      "|       from small pool |   1654 KiB |      5 MiB |     52 GiB |     52 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      | 751617 KiB |   1483 MiB |   2007 GiB |   2006 GiB |\n",
      "|       from large pool | 749968 KiB |   1482 MiB |   1954 GiB |   1953 GiB |\n",
      "|       from small pool |   1649 KiB |      5 MiB |     52 GiB |     52 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    796 MiB |   3336 MiB |   3340 MiB |   2544 MiB |\n",
      "|       from large pool |    794 MiB |   3328 MiB |   3328 MiB |   2534 MiB |\n",
      "|       from small pool |      2 MiB |      8 MiB |     12 MiB |     10 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  59529 KiB | 690296 KiB |   2370 GiB |   2370 GiB |\n",
      "|       from large pool |  59136 KiB | 688126 KiB |   2317 GiB |   2317 GiB |\n",
      "|       from small pool |    393 KiB |   4168 KiB |     52 GiB |     52 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     450    |     450    |  483642    |  483192    |\n",
      "|       from large pool |     162    |     167    |  279798    |  279636    |\n",
      "|       from small pool |     288    |     288    |  203844    |  203556    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     450    |     450    |  483642    |  483192    |\n",
      "|       from large pool |     162    |     167    |  279798    |  279636    |\n",
      "|       from small pool |     288    |     288    |  203844    |  203556    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      24    |      34    |      36    |      12    |\n",
      "|       from large pool |      23    |      30    |      30    |       7    |\n",
      "|       from small pool |       1    |       4    |       6    |       5    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      15    |      61    |  233663    |  233648    |\n",
      "|       from large pool |      14    |      36    |  153318    |  153304    |\n",
      "|       from small pool |       1    |      30    |   80345    |   80344    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='working/base2',\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    # gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    max_steps=10000,\n",
    "    save_steps=1000,  #100,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=100,\n",
    "    learning_rate=3e-5,\n",
    "    warmup_steps=2000,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=timit_dataset['train'],\n",
    "    eval_dataset=timit_dataset['validation'],\n",
    "    tokenizer=processor.feature_extractor\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 100/10000 [01:40<1:53:26,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.1329, 'learning_rate': 1.4850000000000002e-06, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 200/10000 [03:18<1:54:12,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.7518, 'learning_rate': 2.9700000000000004e-06, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 300/10000 [04:59<1:48:17,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.9792, 'learning_rate': 4.455e-06, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 400/10000 [06:40<1:47:25,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6701, 'learning_rate': 5.955000000000001e-06, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 500/10000 [08:24<2:13:55,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1677, 'learning_rate': 7.455e-06, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 600/10000 [10:03<2:12:56,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0909, 'learning_rate': 8.955e-06, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 700/10000 [11:42<2:11:52,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0805, 'learning_rate': 1.0454999999999999e-05, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 800/10000 [13:24<2:13:18,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0625, 'learning_rate': 1.1955000000000002e-05, 'epoch': 1.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 900/10000 [15:05<2:08:37,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.066, 'learning_rate': 1.3455e-05, 'epoch': 1.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1000/10000 [16:48<2:26:08,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1019, 'learning_rate': 1.4955e-05, 'epoch': 2.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 10%|█         | 1000/10000 [17:47<2:26:08,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.30656623840332, 'eval_wer': 1.0, 'eval_cer': 0.9395153568535023, 'eval_runtime': 59.163, 'eval_samples_per_second': 15.618, 'eval_steps_per_second': 1.961, 'epoch': 2.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1100/10000 [19:37<2:26:45,  1.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0376, 'learning_rate': 1.6455e-05, 'epoch': 2.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1200/10000 [21:17<2:23:13,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6583, 'learning_rate': 1.7955e-05, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1300/10000 [22:57<2:21:34,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4458, 'learning_rate': 1.9455e-05, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1400/10000 [24:38<2:44:05,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3952, 'learning_rate': 2.0955000000000002e-05, 'epoch': 3.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 1500/10000 [26:18<2:42:06,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3533, 'learning_rate': 2.2455e-05, 'epoch': 3.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1600/10000 [27:58<2:40:21,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3004, 'learning_rate': 2.3955e-05, 'epoch': 3.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1700/10000 [29:39<2:39:31,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2748, 'learning_rate': 2.5455000000000003e-05, 'epoch': 3.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1800/10000 [31:19<2:36:48,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.24, 'learning_rate': 2.6955e-05, 'epoch': 3.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1900/10000 [32:56<2:41:02,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2146, 'learning_rate': 2.8455e-05, 'epoch': 4.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2000/10000 [34:37<2:43:37,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1848, 'learning_rate': 2.9955000000000004e-05, 'epoch': 4.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 20%|██        | 2000/10000 [35:36<2:43:37,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1350936889648438, 'eval_wer': 1.0, 'eval_cer': 0.7652647860352066, 'eval_runtime': 58.9541, 'eval_samples_per_second': 15.673, 'eval_steps_per_second': 1.968, 'epoch': 4.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2100/10000 [37:25<2:29:54,  1.14s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1346, 'learning_rate': 2.963625e-05, 'epoch': 4.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2200/10000 [39:05<2:33:51,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1042, 'learning_rate': 2.926125e-05, 'epoch': 4.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 2300/10000 [40:45<2:29:07,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0388, 'learning_rate': 2.888625e-05, 'epoch': 4.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 2400/10000 [42:29<1:47:31,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8729, 'learning_rate': 2.851125e-05, 'epoch': 5.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2500/10000 [44:08<1:43:48,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.703, 'learning_rate': 2.813625e-05, 'epoch': 5.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 2600/10000 [45:48<1:41:59,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5233, 'learning_rate': 2.7761249999999998e-05, 'epoch': 5.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 2700/10000 [47:29<1:43:27,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.355, 'learning_rate': 2.738625e-05, 'epoch': 5.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 2800/10000 [49:11<1:54:04,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2066, 'learning_rate': 2.7011250000000003e-05, 'epoch': 6.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 2900/10000 [50:51<1:51:52,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0505, 'learning_rate': 2.663625e-05, 'epoch': 6.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3000/10000 [52:31<1:50:47,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9413, 'learning_rate': 2.6261250000000002e-05, 'epoch': 6.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 30%|███       | 3000/10000 [53:30<1:50:47,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.073375940322876, 'eval_wer': 0.9987786259541984, 'eval_cer': 0.3184650511895117, 'eval_runtime': 58.9286, 'eval_samples_per_second': 15.68, 'eval_steps_per_second': 1.968, 'epoch': 6.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 3100/10000 [55:20<1:45:52,  1.09it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8473, 'learning_rate': 2.588625e-05, 'epoch': 6.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 3200/10000 [56:57<1:47:11,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7361, 'learning_rate': 2.551125e-05, 'epoch': 6.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3300/10000 [58:34<2:00:24,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6325, 'learning_rate': 2.5136250000000002e-05, 'epoch': 7.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 3400/10000 [1:00:13<2:02:13,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5815, 'learning_rate': 2.476125e-05, 'epoch': 7.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 3500/10000 [1:01:51<1:58:58,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5097, 'learning_rate': 2.438625e-05, 'epoch': 7.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 3600/10000 [1:03:26<1:56:23,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4585, 'learning_rate': 2.4011249999999998e-05, 'epoch': 7.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 3700/10000 [1:05:02<2:23:27,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.419, 'learning_rate': 2.363625e-05, 'epoch': 8.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3800/10000 [1:06:38<2:15:45,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3904, 'learning_rate': 2.326125e-05, 'epoch': 8.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 3900/10000 [1:08:17<2:09:11,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3687, 'learning_rate': 2.288625e-05, 'epoch': 8.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4000/10000 [1:09:54<2:12:12,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3321, 'learning_rate': 2.2515e-05, 'epoch': 8.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 40%|████      | 4000/10000 [1:10:53<2:12:12,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4974331855773926, 'eval_wer': 0.8934351145038167, 'eval_cer': 0.13082418796494072, 'eval_runtime': 58.369, 'eval_samples_per_second': 15.83, 'eval_steps_per_second': 1.987, 'epoch': 8.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 4100/10000 [1:12:41<2:06:00,  1.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3195, 'learning_rate': 2.214e-05, 'epoch': 8.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 4200/10000 [1:14:21<1:16:51,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2782, 'learning_rate': 2.1765000000000003e-05, 'epoch': 9.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 4300/10000 [1:16:01<1:15:58,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3225, 'learning_rate': 2.139e-05, 'epoch': 9.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4400/10000 [1:17:40<1:12:42,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2538, 'learning_rate': 2.1015e-05, 'epoch': 9.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 4500/10000 [1:19:22<1:14:41,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2636, 'learning_rate': 2.064e-05, 'epoch': 9.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 4600/10000 [1:21:02<1:10:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2337, 'learning_rate': 2.0265e-05, 'epoch': 9.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 4700/10000 [1:22:46<1:23:49,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2252, 'learning_rate': 1.989e-05, 'epoch': 10.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 4800/10000 [1:24:27<1:19:27,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1996, 'learning_rate': 1.9515e-05, 'epoch': 10.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 4900/10000 [1:26:08<1:19:54,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2235, 'learning_rate': 1.914e-05, 'epoch': 10.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5000/10000 [1:27:47<1:18:50,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.203, 'learning_rate': 1.8764999999999997e-05, 'epoch': 10.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 50%|█████     | 5000/10000 [1:28:47<1:18:50,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42782527208328247, 'eval_wer': 0.8650381679389313, 'eval_cer': 0.12039478529866686, 'eval_runtime': 59.4975, 'eval_samples_per_second': 15.53, 'eval_steps_per_second': 1.95, 'epoch': 10.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 5100/10000 [1:30:39<1:29:53,  1.10s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1987, 'learning_rate': 1.8390000000000002e-05, 'epoch': 11.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 5200/10000 [1:32:19<1:26:54,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1562, 'learning_rate': 1.8015000000000003e-05, 'epoch': 11.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 5300/10000 [1:33:58<1:23:43,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1584, 'learning_rate': 1.764e-05, 'epoch': 11.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5400/10000 [1:35:39<1:22:55,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.161, 'learning_rate': 1.7265e-05, 'epoch': 11.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 5500/10000 [1:37:20<1:22:27,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.153, 'learning_rate': 1.689e-05, 'epoch': 11.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5600/10000 [1:39:00<1:37:54,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.157, 'learning_rate': 1.6515e-05, 'epoch': 12.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 5700/10000 [1:40:40<1:34:07,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1398, 'learning_rate': 1.614e-05, 'epoch': 12.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 5800/10000 [1:42:20<1:28:59,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1478, 'learning_rate': 1.5772500000000002e-05, 'epoch': 12.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 5900/10000 [1:43:59<1:31:00,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1233, 'learning_rate': 1.53975e-05, 'epoch': 12.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6000/10000 [1:45:39<1:11:17,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1105, 'learning_rate': 1.5026249999999999e-05, 'epoch': 12.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 60%|██████    | 6000/10000 [1:46:39<1:11:17,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3745562434196472, 'eval_wer': 0.8430534351145038, 'eval_cer': 0.11553362303896295, 'eval_runtime': 59.5589, 'eval_samples_per_second': 15.514, 'eval_steps_per_second': 1.948, 'epoch': 12.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 6100/10000 [1:48:31<50:38,  1.28it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1149, 'learning_rate': 1.465125e-05, 'epoch': 13.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 6200/10000 [1:50:12<48:48,  1.30it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1105, 'learning_rate': 1.427625e-05, 'epoch': 13.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 6300/10000 [1:51:52<47:05,  1.31it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0935, 'learning_rate': 1.390125e-05, 'epoch': 13.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 6400/10000 [1:53:32<45:59,  1.30it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0935, 'learning_rate': 1.3526250000000001e-05, 'epoch': 13.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6500/10000 [1:55:16<54:59,  1.06it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1026, 'learning_rate': 1.315125e-05, 'epoch': 14.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 6600/10000 [1:56:55<51:01,  1.11it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.068, 'learning_rate': 1.277625e-05, 'epoch': 14.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6700/10000 [1:58:35<48:54,  1.12it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0844, 'learning_rate': 1.2401250000000001e-05, 'epoch': 14.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 6800/10000 [2:00:17<50:29,  1.06it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0731, 'learning_rate': 1.202625e-05, 'epoch': 14.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 6900/10000 [2:01:57<48:28,  1.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0919, 'learning_rate': 1.1651250000000001e-05, 'epoch': 14.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7000/10000 [2:03:38<52:39,  1.05s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0808, 'learning_rate': 1.127625e-05, 'epoch': 15.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 70%|███████   | 7000/10000 [2:04:37<52:39,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3602787256240845, 'eval_wer': 0.8225954198473282, 'eval_cer': 0.11198350150990646, 'eval_runtime': 59.3075, 'eval_samples_per_second': 15.58, 'eval_steps_per_second': 1.956, 'epoch': 15.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 7100/10000 [2:06:28<52:13,  1.08s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0739, 'learning_rate': 1.090125e-05, 'epoch': 15.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 7200/10000 [2:08:08<49:19,  1.06s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.057, 'learning_rate': 1.052625e-05, 'epoch': 15.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 7300/10000 [2:09:49<47:38,  1.06s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0781, 'learning_rate': 1.015125e-05, 'epoch': 15.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 7400/10000 [2:11:28<58:52,  1.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0728, 'learning_rate': 9.776250000000001e-06, 'epoch': 16.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 7500/10000 [2:13:09<51:09,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0356, 'learning_rate': 9.40125e-06, 'epoch': 16.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 7600/10000 [2:14:47<50:46,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.053, 'learning_rate': 9.02625e-06, 'epoch': 16.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 7700/10000 [2:16:28<49:09,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0575, 'learning_rate': 8.65125e-06, 'epoch': 16.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7800/10000 [2:18:09<46:42,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0463, 'learning_rate': 8.276249999999999e-06, 'epoch': 16.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 7900/10000 [2:19:47<25:35,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.012, 'learning_rate': 7.901250000000002e-06, 'epoch': 17.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8000/10000 [2:21:27<24:14,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0657, 'learning_rate': 7.526250000000001e-06, 'epoch': 17.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 80%|████████  | 8000/10000 [2:22:26<24:14,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34905609488487244, 'eval_wer': 0.8079389312977099, 'eval_cer': 0.11042203726891066, 'eval_runtime': 59.2346, 'eval_samples_per_second': 15.599, 'eval_steps_per_second': 1.958, 'epoch': 17.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 8100/10000 [2:24:17<23:53,  1.33it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0138, 'learning_rate': 7.15125e-06, 'epoch': 17.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 8200/10000 [2:25:56<21:48,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0445, 'learning_rate': 6.77625e-06, 'epoch': 17.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 8300/10000 [2:27:38<21:30,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0218, 'learning_rate': 6.401250000000001e-06, 'epoch': 17.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 8400/10000 [2:29:22<24:59,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.021, 'learning_rate': 6.02625e-06, 'epoch': 18.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 8500/10000 [2:31:00<22:12,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0521, 'learning_rate': 5.6512499999999995e-06, 'epoch': 18.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 8600/10000 [2:32:39<20:34,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0468, 'learning_rate': 5.2762500000000005e-06, 'epoch': 18.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 8700/10000 [2:34:20<19:09,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0526, 'learning_rate': 4.90125e-06, 'epoch': 18.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 8800/10000 [2:36:01<20:46,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0275, 'learning_rate': 4.52625e-06, 'epoch': 19.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8900/10000 [2:37:41<19:01,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0284, 'learning_rate': 4.15125e-06, 'epoch': 19.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8942/10000 [2:38:23<20:26,  1.16s/it]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=768, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"working/base/checkpoint-7000-final\",\n",
    "    attention_dropout=0.1,\n",
    "    layerdrop=0.0,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.75,\n",
    "    mask_time_length=10,\n",
    "    mask_feature_prob=0.25,\n",
    "    mask_feature_length=64,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "model.freeze_feature_extractor()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 376050 KiB | 376050 KiB | 381442 KiB |   5392 KiB |\n",
      "|       from large pool | 375040 KiB | 375040 KiB | 375040 KiB |      0 KiB |\n",
      "|       from small pool |   1010 KiB |   5563 KiB |   6402 KiB |   5392 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 376050 KiB | 376050 KiB | 381442 KiB |   5392 KiB |\n",
      "|       from large pool | 375040 KiB | 375040 KiB | 375040 KiB |      0 KiB |\n",
      "|       from small pool |   1010 KiB |   5563 KiB |   6402 KiB |   5392 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      | 374525 KiB | 374525 KiB | 379917 KiB |   5392 KiB |\n",
      "|       from large pool | 373520 KiB | 373520 KiB | 373520 KiB |      0 KiB |\n",
      "|       from small pool |   1005 KiB |   5559 KiB |   6397 KiB |   5392 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   | 430080 KiB | 434176 KiB | 434176 KiB |   4096 KiB |\n",
      "|       from large pool | 428032 KiB | 428032 KiB | 428032 KiB |      0 KiB |\n",
      "|       from small pool |   2048 KiB |   6144 KiB |   6144 KiB |   4096 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  54029 KiB |  54174 KiB | 287743 KiB | 233714 KiB |\n",
      "|       from large pool |  52992 KiB |  52992 KiB | 277744 KiB | 224752 KiB |\n",
      "|       from small pool |   1037 KiB |   4168 KiB |   9999 KiB |   8962 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     236    |     236    |     268    |      32    |\n",
      "|       from large pool |      81    |      81    |      81    |       0    |\n",
      "|       from small pool |     155    |     155    |     187    |      32    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     236    |     236    |     268    |      32    |\n",
      "|       from large pool |      81    |      81    |      81    |       0    |\n",
      "|       from small pool |     155    |     155    |     187    |      32    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      22    |      24    |      24    |       2    |\n",
      "|       from large pool |      21    |      21    |      21    |       0    |\n",
      "|       from small pool |       1    |       3    |       3    |       2    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      19    |      19    |      26    |       7    |\n",
      "|       from large pool |      18    |      18    |      20    |       2    |\n",
      "|       from small pool |       1    |       4    |       6    |       5    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='working/base/eval',\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    # gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    max_steps=10000,\n",
    "    save_steps=1000,  #100,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=100,\n",
    "    learning_rate=3e-5,\n",
    "    warmup_steps=2000,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=timit_dataset['train'],\n",
    "    eval_dataset=timit_dataset['validation'],\n",
    "    tokenizer=processor.feature_extractor\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [01:43<00:00,  2.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 17.085832595825195,\n",
       " 'eval_wer': 1.100810939736047,\n",
       " 'eval_cer': 1.2895882947835449,\n",
       " 'eval_runtime': 104.7644,\n",
       " 'eval_samples_per_second': 16.036,\n",
       " 'eval_steps_per_second': 2.004}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(timit_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [03:46<00:00,  2.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 16.69365119934082,\n",
       " 'eval_wer': 1.103370954480903,\n",
       " 'eval_cer': 1.2620423750326513,\n",
       " 'eval_runtime': 227.2737,\n",
       " 'eval_samples_per_second': 16.262,\n",
       " 'eval_steps_per_second': 2.033}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(timit_dataset['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "song-transcription",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
